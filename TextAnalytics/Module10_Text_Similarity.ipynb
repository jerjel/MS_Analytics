{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics | BAIS:6100\n",
    "# Module 10: Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor: Kang-Pyo Lee \n",
    "\n",
    "Topics to be covered:\n",
    "- Word similarity (+ exercises)\n",
    "- Text similarity (+ exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text similarity is a number typically between 0 and 1 that tells us how close two pieces of text are. Text similarity has many applications. It can be used by search engines in modeling the relevance of a document to a search query and in expanding search terms with their similar terms. It can be used by recommendation engines that maintain lists of similar words or terms. It can be used to detect plagiarism between two documents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"text\"\n",
    "word2 = \"tests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance, also known as edit distance, is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"classdata/images/levenshtein_distance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_levenshtein_dist(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y), dtype=int)\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    \n",
    "    return (matrix[size_x-1, size_y-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtein Distance and Text Similarity in Python: https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_levenshtein_dist(\"text\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_levenshtein_dist(\"text\", \"texts\")   # one addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_levenshtein_dist(\"text\", \"test\")    # one substitution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_levenshtein_dist(\"text\", \"tests\")   # one substitution + one addition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_levenshtein_dist(\"text\", \"texture\") # three additions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_levenshtein_dist(\"text\", \"toast\")   # two substitutions + one addition   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = \"covid19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "months = [\"202012\", \"202011\", \"202010\", \"202009\", \"202008\", \"202007\", \n",
    "          \"202006\", \"202005\", \"202004\", \"202003\", \"202002\", \"202001\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for month in months:\n",
    "    dftmp = pd.read_csv(\"classdata/tweets/tweets_{}_{}.csv\".format(hashtag, month), sep=\"\\t\", quoting=3)\n",
    "    \n",
    "    ##############################################\n",
    "    # Create a random sample of N rows.\n",
    "    ##############################################\n",
    "    if len(dftmp) > N:\n",
    "        dftmp = dftmp.sample(n=N)\n",
    "    ##############################################\n",
    "    \n",
    "    df = pd.concat([df, dftmp])\n",
    "    print(\"{}: {:,}\".format(month, len(dftmp)))\n",
    "\n",
    "print(\"Total number of tweets in df: {:,}\\n\".format(len(df)))\n",
    "\n",
    "df.user_name = df.user_name.astype(str)\n",
    "df.text = df.text.astype(str)\n",
    "df = df.drop_duplicates([\"text\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def get_unique_words(text_list):\n",
    "    all_words = set()\n",
    "    \n",
    "    for text in text_list:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        for word in words:\n",
    "            if re.search(\"^[a-zA-Z][a-zA-Z0-9]+\", word):  # Any word starting with an alphabet letter followed by any alphanumerical characters\n",
    "                all_words.add(word.lower())\n",
    "                \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = get_unique_words(df.text[:100])\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = sorted(unique_words)     # A set becomes a list when sorted.\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1 in unique_words:\n",
    "    if (len(word1) < 5) | (word1.endswith(\"…\")):      # Filter out words that are too short or end with '…'\n",
    "        continue\n",
    "        \n",
    "    for word2 in unique_words:\n",
    "        if (len(word2) < 5) | (word2.endswith(\"…\")):  # Filter out words that are too short or end with '…'\n",
    "            continue\n",
    "            \n",
    "        if word1 != word2:\n",
    "            dist = get_levenshtein_dist(word1, word2)\n",
    "            \n",
    "            if dist == 1:\n",
    "                print(word1, word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance between any two words is symmetric. Two words with edit distance 1 are mostly word variations such as singular and plural nouns. Some of them are not similar at all such as bears and years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "for word1 in unique_words:\n",
    "    if (len(word1) < 5) | (word1.endswith(\"…\")):      # Filter out words that are too short or end with '…'\n",
    "        continue\n",
    "        \n",
    "    for word2 in unique_words:\n",
    "        if (len(word2) < 5) | (word2.endswith(\"…\")):  # Filter out words that are too short or end with '…'\n",
    "            continue\n",
    "            \n",
    "        if (word1 != word2) & (stemmer.stem(word1) == stemmer.stem(word2)):\n",
    "            print(word1, word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises - Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Hey, how are you?\"\n",
    "text2 = \"How are you doing today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity is defined as size of intersection divided by size of union of two sets, in other words, intersection over union. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"classdata/images/sim_jaccard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word not in string.punctuation]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"Hey, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(text1, text2): \n",
    "    a = set(tokenize(text1)) \n",
    "    b = set(tokenize(text2))\n",
    "    i = a & b\n",
    "    u = a | b\n",
    "    \n",
    "    return len(i) / len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_jaccard_sim(text1, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. The closer the angle of two vectors, the higher the cosine similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://d3i71xaburhd42.cloudfront.net/d203f5734f5ee9d49c0adff31805ed93034ca60e/3-Figure1-1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://neo4j.com/docs/graph-algorithms/current/images/cosine-similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_sim(text1, text2):\n",
    "    corpus = [text1, text2]\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None)\n",
    "    dtm = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    return cosine_similarity(dtm)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.metrics.pairwise.cosine_similarity: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cosine similarity, we need to convert sentences into vectors. The choice of TF or TF-IDF depends on application and is immaterial to how cosine similarity is actually performed — which just needs vectors. TF is good for text similarity in general, but TF-IDF is good for search query relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>cosine_similarity</b> module returns an array of the pairwise similarties between all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "global_stopwords = stopwords.words(\"english\")\n",
    "local_stopwords = [c for c in string.punctuation] +\\\n",
    "                  ['’', '``', '…', '...', \"''\", '‘', '“', '”', \"'m\", \"'re\", \"'s\", \"'ve\", 'amp', 'https', \"n't\", 'rt', \n",
    "                   'covid19', 'coronavirus', 'covid19…', 'covid', 'co']\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True, norm=\"l2\", stop_words=global_stopwords+local_stopwords, max_df=0.7)\n",
    "dtm = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(dtm).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = pd.DataFrame(data=cosine_similarity(dtm),\n",
    "                      columns=df.index, index=df.index)\n",
    "df_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values on the diagonal are 1.0 as the similarity of something to itself is always 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pos1 in df_sim.index:\n",
    "    for pos2 in df_sim.index:\n",
    "        if pos1 != pos2:\n",
    "            text1 = df.text[pos1]\n",
    "            text2 = df.text[pos2]\n",
    "            \n",
    "            # Skip if the first 20 characters of one text are in the other text\n",
    "            if (text1[:20] in text2) | (text2[:20] in text1):\n",
    "                continue\n",
    "            \n",
    "            sim = df_sim[pos2][pos1]\n",
    "            \n",
    "            if (sim > 0.7) & (sim < 1):\n",
    "                print(text1)\n",
    "                print(text2)\n",
    "                print(\"{:.3f}\".format(sim))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between Jaccard Similarity and Cosine Similarity:\n",
    "- Jaccard similarity takes only unique set of words for each sentence/document while cosine similarity takes total length of the vectors. \n",
    "- Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. For two product descriptions, it will be better to use Jaccard similarity as repetition of a word does not reduce their similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Text Similarity Metrics in Python: https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises - Text Similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

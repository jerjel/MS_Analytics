{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Programming in Python | BAIS:6040\n",
    "# Data Manipulation & Dates in Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(data = np.random.normal(0,1,(100,2))\n",
    "                  ,columns = ['Num1','Num2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a Boolean Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Num1Larger'] = df.Num1 > df.Num2\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add New Column Using Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Difference'] = abs(df.Num1-df.Num2)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add New Column Using List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Difference2'] = [abs(df.Num1[i]-df.Num2[i]) for i in range(len(df))]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add New Conditional as String\n",
    "\n",
    "https://numpy.org/doc/1.18/reference/generated/numpy.where.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['LargerCategory'] = np.where(df.Num1 >= df.Num2, \"Num1Bigger\", \"Num2Bigger\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add New Conditional Column as Pandas Category\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Categorical.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LargerCategory'] = pd.Categorical(np.where(df.Num1 >= df.Num2, \"Num1Bigger\", \"Num2Bigger\"))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Widening the Data into Tidy / Panel Data \n",
    "\n",
    "- Oftentimes you have data that is not in a Tidy or Panel Data format.\n",
    "- This generally happens when using SQL to query data from a relational database that is normalized.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tidy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'pie_rates.csv')\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas pivot_table - Similar to spread() in R\n",
    "\n",
    "- Organizing the data around the type of pie with a separate Rate column for each Country\n",
    "- Reset the index in-place so that Pie is it's own column and not an index\n",
    "- Setting the row and column indices to None so they don't display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing the data around the type of pie with a separate Rate column for each Country\n",
    "df1=pd.pivot_table(df,index='Pie',columns='Country',values='Rate')\n",
    "\n",
    "# reset the index in-place so that Pie is it's own column and not an index\n",
    "df1.reset_index(inplace=True)\n",
    "\n",
    "# setting the row and column indices to None so they don't display\n",
    "df1.index.name = None\n",
    "df1.columns.name = None\n",
    "\n",
    "# notice the null values created\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Narrowing the Data \n",
    "#### Pandas Melt - Similar to gather() in R\n",
    "\n",
    "- Gather the data back to the original so that Pie and Country are columns\n",
    "- Drop the rows with nulls (in-place) to get back to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gathering the data back to the original so that Pie and Country are columns\n",
    "df2=pd.melt(df1, id_vars='Pie', var_name ='Country', value_name='Rate')\n",
    "\n",
    "# notice the null values are still present\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with nulls (in-place) to get back to the original\n",
    "df2.dropna(inplace=True)\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Pie Rates Dataset with a Date Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'pie_rates2.csv')\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df.index.name = None\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Aggregations When Widening\n",
    "* Can use several aggregation functions like mean, median, max, min\n",
    "* Several options for which columns you want to persist and what you want the index column to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean Rate by Date in each Country across all Pies\n",
    "df1=pd.pivot_table(df,index='Date',columns='Country',values='Rate',aggfunc=np.mean)\n",
    "\n",
    "# reset the index in-place\n",
    "df1.reset_index(inplace=True)\n",
    "\n",
    "# setting the row and column indices to None so they don't display\n",
    "df1.index.name = None\n",
    "df1.columns.name = None\n",
    " \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean Rate by Date for each Pie across all Countries\n",
    "df1=pd.pivot_table(df,index='Date',columns='Pie',values='Rate',aggfunc=np.mean)\n",
    "\n",
    "# reset the index in-place\n",
    "df1.reset_index(inplace=True)\n",
    "\n",
    "# setting the row and column indices to None so they don't display\n",
    "df1.index.name = None\n",
    "df1.columns.name = None\n",
    " \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean Rate by Pie in each Country, over all Dates\n",
    "df1=pd.pivot_table(df,index='Pie',columns='Country',values='Rate',aggfunc=np.mean)\n",
    "\n",
    "# reset the index in-place\n",
    "df1.reset_index(inplace=True)\n",
    "\n",
    "# setting the row and column indices to None so they don't display\n",
    "df1.index.name = None\n",
    "df1.columns.name = None\n",
    " \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean Rate by Country for each Pie, over all Dates\n",
    "df1=pd.pivot_table(df,index='Country',columns='Pie',values='Rate',aggfunc=np.mean)\n",
    "\n",
    "# reset the index in-place\n",
    "#df1.reset_index(inplace=True)\n",
    "\n",
    "# setting the row and column indices to None so they don't display\n",
    "df1.index.name = None\n",
    "df1.columns.name = None\n",
    " \n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilevel Indices\n",
    "* Axis 1 now has two indices; one for Country, and one for Pie\n",
    "* Note: You can not change the index names to None.  They are Frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean Rate by Date in each Country for each Pie\n",
    "df1=pd.pivot_table(df,index='Date',columns=['Country','Pie'],values='Rate',aggfunc=np.mean)\n",
    "\n",
    "# reset the index in-place\n",
    "#df1.reset_index(inplace=True)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending & Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame([100,200,300,400]\n",
    "                    ,index=['a','b','c','d']\n",
    "                    ,columns=['A'])\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame([200,150,500]\n",
    "                    ,index=['f','b','d']\n",
    "                    ,columns=['B'])\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append data from df2 to df1 as new rows\n",
    "\n",
    "- Returns a new dataframe, does not change df1\n",
    "- Because the two dataframes do not have the same column indices (columns), it creates NaN's\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.append(df2, sort=False)  # keeps the same row indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.append(df2, sort=False, ignore_index=True)  # ignores the existing row indices and creates new default indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate two dataframes along the row axis (axis = 0)\n",
    "\n",
    "- Same result as using append\n",
    "- Because the two dataframes do not have the same column indices (columns), it creates NaN's\n",
    "- axis = 0 is the default\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat((df1,df2), sort=False)  # keeps the same row indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat((df1,df2), sort=False, ignore_index=True)  # ignores the existing row indices and creates new default indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append & Concatenate data from df2 to df1 as new rows\n",
    "\n",
    "- Returns a new dataframe, does not change df1\n",
    "- Because the two dataframes have the <b>column indices (columns)</b>, it does NOT create NaN's\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df1 = pd.DataFrame(data = np.random.normal(10,2,(100,2))\n",
    "                  ,columns = ['Num1','Num2'])\n",
    "\n",
    "df2 = pd.DataFrame(data = np.random.normal(0,1,(10,2))\n",
    "                  ,columns = ['Num1','Num2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.append(df2, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2])  # can pass a list of dataframes like here or a tuple like above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate two dataframes along the column axis (axis = 1)\n",
    "\n",
    "- Returns a new dataframe with columns of each dataframe\n",
    "- If the two dataframes have the same row indices, it does not create NaN's by default\n",
    "- You can specify the type of join. If outer join, then if all the row indices don't match, it creates NaN's\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df1 = pd.DataFrame(data = np.random.normal(10,2,(10,2))\n",
    "                  ,columns = ['Num1','Num2'])\n",
    "\n",
    "df2 = pd.DataFrame(data = np.random.normal(0,1,(10,2))\n",
    "                  ,columns = ['Num3','Num4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2], sort=False, axis=1, join='inner')  # all the row indices match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df1 = pd.DataFrame(data = np.random.normal(10,2,(10,2))\n",
    "                  ,columns = ['Num1','Num2'])\n",
    "\n",
    "df2 = pd.DataFrame(data = np.random.normal(0,1,(5,2))\n",
    "                  ,columns = ['Num3','Num4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2], sort=False, axis=1)  # not all row indices match and join = 'outer' by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2], sort=False, axis=1, join='inner')  # not all row indices match, but forcing inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining\n",
    "\n",
    "- Join two dataframes similiar to joining two tables in SQL\n",
    "- Can specify the columns to 'join on' or default and use row indices\n",
    "- Joins can be left, inner, or outer : left is the default\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame(data = np.random.normal(10,2,(10,2))\n",
    "                  ,columns = ['Num1','Num2'])\n",
    "\n",
    "df2 = pd.DataFrame(data = np.random.normal(0,1,(5,2))\n",
    "                  ,columns = ['Num3','Num4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.join(df2)  # the rows from df1 are the base and left join to df2 creates NaN's where row indices don't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.join(df1)  # the rows from df2 are the base and left join to df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.join(df2, how='inner')  # only the rows with matching indices are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, how='outer')  # all rows from both dataframes returned with matching indices consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "- Typically takes place on a column that is shared between dataframes\n",
    "- Can, however, be used in place of Join with columns that do not match\n",
    "- The 'how' parameter works the same as in Join\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df1 = pd.DataFrame(data = np.random.normal(10,2,(10,2))\n",
    "                  ,columns = ['Num1','Num2'])\n",
    "\n",
    "df2 = pd.DataFrame(data = np.random.normal(0,1,(5,2))\n",
    "                  ,columns = ['Num3','Num4'])\n",
    "\n",
    "num5 = pd.Series(np.arange(0, 10, 1))\n",
    "\n",
    "df1['Num5'] = num5\n",
    "df2['Num5'] = num5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)  # matches on column and default 'how' is inner on row indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2, how='left')  # matches on column and left join 'how' on row indices creates NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2, how='outer')  # matches on column and outer join 'how' includes all row indices  & creates NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of the dataframes dictates order of columns in the output\n",
    "pd.merge(df2,df1, how='outer')  # matches on column and outer join 'how' includes all row indices  & creates NaN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Dates in Pandas\n",
    "* index_col parameter sets the index to the column reference\n",
    "* parse_dates parameter indicates which column(s) to try and create date objects from\n",
    "* squeeze parameter indicates whether to represent as a single series if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'daily-minimum-temperatures.csv')\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the data types - Date column is just an Object\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'daily-minimum-temperatures.csv')\n",
    "\n",
    "# using the parse_dates argument and indicating what column to parse\n",
    "df = pd.read_csv(filepath, parse_dates=['Date'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the data types - Date column is now a datetime object\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Additional Parameters on the Load\n",
    "\n",
    "- Setting the index_col parameter to the column that should be in the index. In this case Date\n",
    "- Setting the parse_dates to a boolean instead of a column indicates the operation should apply to the index\n",
    "- Setting the squeeze parameter to True squeezes the one column dataframe down to type Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'daily-minimum-temperatures.csv')\n",
    "\n",
    "# setting the index_col parameter to the column that should be in the index. In this case Date\n",
    "# setting the parse_dates to a boolean instead of a column indicates the operation should apply to the index\n",
    "df = pd.read_csv(filepath,header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the data types - the index is a datetime object\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'daily-minimum-temperatures.csv')\n",
    "\n",
    "# setting the squeeze parameter to True squeezes the one column dataframe down to type Series\n",
    "df = pd.read_csv(filepath,header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice what class df is\n",
    "print(type(df))\n",
    "\n",
    "# notice the dtype of the index of the Series\n",
    "df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Dataset with Timestamp values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'sampleTs.csv')\n",
    "\n",
    "# parsing the TimeStamp column as a date\n",
    "df = pd.read_csv(filepath, parse_dates=['TimeStamp'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the dtype of the TimeStamp column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each datetime object has the following properties\n",
    "* month\n",
    "* year\n",
    "* hour\n",
    "* day\n",
    "* minute\n",
    "* second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Month\")\n",
    "print(df.TimeStamp.dt.month.head(2))\n",
    "print(\"Year\")\n",
    "print(df.TimeStamp.dt.year.head(2))\n",
    "print(\"Hour\")\n",
    "print(df.TimeStamp.dt.hour.head(2))\n",
    "print(\"Day\")\n",
    "print(df.TimeStamp.dt.day.head(2))\n",
    "print(\"Minute\")\n",
    "print(df.TimeStamp.dt.minute.head(2))\n",
    "print(\"Second\")\n",
    "print(df.TimeStamp.dt.second.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can also find the minimum and maximum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum Month: ',df.TimeStamp.dt.month.max())\n",
    "\n",
    "print(\"Minimum Timestamp: \",df.TimeStamp.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TimeStamp.dt.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "* Add Month, Day, Year, Hour, Weekday (as integer), and BusDay (as boolean) Features\n",
    "* Using List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['Month'] = [df.TimeStamp[i].month for i in range(len(df.TimeStamp))]\n",
    "df['Day'] = [df.TimeStamp[i].day for i in range(len(df.TimeStamp))]\n",
    "df['Year'] = [df.TimeStamp[i].year for i in range(len(df.TimeStamp))]\n",
    "df['Hour'] = [df.TimeStamp[i].hour for i in range(len(df.TimeStamp))]\n",
    "df['Weekday'] = [df.TimeStamp.dt.date[i].weekday() for i in range(len(df.TimeStamp.dt.date))]\n",
    "df['BusDay'] = np.is_busday(df.TimeStamp.dt.date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['Month'] = [df.TimeStamp[i].month for i in range(len(df.TimeStamp))]\n",
    "df['Day'] = [df.TimeStamp[i].day for i in range(len(df.TimeStamp))]\n",
    "df['Year'] = [df.TimeStamp[i].year for i in range(len(df.TimeStamp))]\n",
    "df['Hour'] = [df.TimeStamp[i].hour for i in range(len(df.TimeStamp))]\n",
    "df['Weekday'] = [df.TimeStamp.dt.date[i].weekday() for i in range(len(df.TimeStamp.dt.date))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['Month2'] = df.TimeStamp.dt.month\n",
    "df['Day2'] = df.TimeStamp.dt.day\n",
    "df['Year2'] = df.TimeStamp.dt.year\n",
    "df['Hour2'] = df.TimeStamp.dt.hour\n",
    "df['Weekday2'] = df.TimeStamp.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add WeekdayText Feature as a Categorical\n",
    "* Using List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# weekdays as a tuple\n",
    "# Monday is day 0\n",
    "days = (\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\")\n",
    "\n",
    "df['WeekdayText'] = pd.Categorical([days[df.TimeStamp.dt.date[i].weekday()] for i in range(len(df.TimeStamp.dt.date))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using day_name() & month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['WeekdayText2'] = pd.Categorical(df.TimeStamp.dt.day_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WeekdayText3'] = pd.Categorical(df.TimeStamp.dt.strftime('%A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MonthText'] = df.TimeStamp.dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Qtr and QtrText Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrs = (\"First\",\"Second\",\"Third\",\"Fourth\")\n",
    "df['Qtr'] = pd.Categorical(df.TimeStamp.dt.quarter)\n",
    "df['QtrText'] = pd.Categorical([qtrs[df.Qtr[i]-1] for i in range(len(df.TimeStamp.dt.date))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the dtype of WeekdayText compared to MonthText\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Time Zones of a datetime\n",
    "* First step is to localize - which means defining what time zone the original data is in\n",
    "* A timezone's offset refers to how many hours the timezone is from Coordinated Universal Time (UTC)\n",
    "* A naive datetime object contains no timezone information. \n",
    "* Check tzinfo of a datetime object. tzinfo will be set to None if the object is naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'sampleTs.csv')\n",
    "\n",
    "# parsing the TimeStamp column as a date\n",
    "df = pd.read_csv(filepath, parse_dates=['TimeStamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the datetime is naive\n",
    "print(df.TimeStamp[0].tzinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# localizing to US Central Time Zone\n",
    "df['TimeStamp'] = df.TimeStamp.dt.tz_localize('US/Central')\n",
    "\n",
    "# now the datetime is aware\n",
    "df.TimeStamp[0].tzinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the attributes of TimeStamp\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new datetime column that converts the original timestamp to UTC\n",
    "df['TimeStamp_UTC'] = df['TimeStamp'].dt.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a datetime index using date_range\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 hour frequency\n",
    "pd.date_range(start = '2019-01-01', end = '2019-01-02', freq = '3H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 day frequency\n",
    "pd.date_range(start = '2019-01-01', end = '2019-01-05', freq = 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual on first day of year for 10 years\n",
    "pd.date_range(start = '2019-01-01', periods = 10, freq = 'AS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to the dataframe that is a datetime index with a 3 hour interval\n",
    "df['NewDatetime'] = pd.date_range(start = '2019-01-01', periods = len(df), freq = '3H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Programming in Python | BAIS:6040\n",
    "# Time Series Analysis\n",
    "\n",
    "Instructor: Jeff Hendricks\n",
    "\n",
    "Topics to be covered:\n",
    "- Analyzing time series data\n",
    "- Adding lag features and rolling statistics for time series data\n",
    "- Time series stationarity & decomposition\n",
    "\n",
    "References: \n",
    "- Python for Finance (https://learning.oreilly.com/library/view/python-for-finance/9781492024323/)\n",
    "- How to Check if Time Series Data is Stationary with Python by Jason Brownlee (https://machinelearningmastery.com/time-series-data-stationary-python/)\n",
    "\n",
    "- How to Decompose Time Series Data into Trend and Seasonality by Jason Brownlee (https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pfg_data = pd.read_csv('data/PFG.csv',header=0, index_col=0, parse_dates=True)\n",
    "pfg_data.columns = ['Open','High','Low','Close','AdjClose','Volume']\n",
    "\n",
    "pfg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yfinance --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as pdr\n",
    "import yfinance as yf\n",
    "\n",
    "yf.pdr_override()\n",
    "pfg_data = pdr.get_data_yahoo('PFG')#, start, end)\n",
    "\n",
    "pfg_data.columns = ['Open','High','Low','Close','AdjClose','Volume']\n",
    "\n",
    "pfg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pfg_data.plot(figsize=(10,12), subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff() provides absolute changes between two values\n",
    "pfg_data.diff().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_data.diff().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# percent changes rounded to 3 decimal places\n",
    "pfg_data.pct_change().round(3).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mean value of percent changes as bar plot\n",
    "\n",
    "pfg_data[['Open','High','Low','Close','AdjClose']].pct_change().mean().plot(kind='bar', figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_data.AdjClose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shift() provides a lag value for a given index\n",
    "pfg_data.AdjClose.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_data.AdjClose.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using a -1 in shift provides a look-ahead value for a given index\n",
    "pfg_data.AdjClose.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log returns\n",
    "\n",
    "rets = np.log(pfg_data.AdjClose/pfg_data.AdjClose.shift(1))\n",
    "rets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative log returns over time\n",
    "# np.exp needed to convert from log returns\n",
    "rets.cumsum().apply(np.exp).plot(figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.last.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# last() select final periods of time series data based on a date offset.\n",
    "# downsample to 1 week\n",
    "\n",
    "pfg_data.resample('1w', label='right').last().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# downsample to 1 month\n",
    "\n",
    "pfg_data.resample('1m', label='right').last().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cumulative monthly returns\n",
    "\n",
    "rets.cumsum().apply(np.exp).resample('1m', label='right').last().plot(figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Lag Features to Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_close = pd.DataFrame(pfg_data.AdjClose)\n",
    "pfg_close.columns = ['Price']\n",
    "pfg_close.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_close['1 Day Lag'] =  pfg_close.Price.shift(1)\n",
    "pfg_close['7 Day Lag'] =  pfg_close.Price.shift(7)\n",
    "pfg_close['30 Day Lag'] =  pfg_close.Price.shift(30)\n",
    "\n",
    "pfg_close.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Statistics\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window defines the number of index values to include in the rolling statistics\n",
    "win = 10\n",
    "pfg_close['Min'] = pfg_close.Price.rolling(window=win).min()\n",
    "pfg_close['Mean'] = pfg_close.Price.rolling(window=win).mean()\n",
    "pfg_close['Max'] = pfg_close.Price.rolling(window=win).max()\n",
    "pfg_close['StdDev'] = pfg_close.Price.rolling(window=win).std()\n",
    "\n",
    "# EWMA is the exponentially weighted moving average\n",
    "# this is using decay of halflife 0.5\n",
    "pfg_close['EWMA'] = pfg_close.Price.ewm(halflife=0.5, min_periods=win).mean()\n",
    "\n",
    "pfg_close.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pfg_close.dropna().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot some rolling stats and actual price for the last 200 days\n",
    "\n",
    "ax = pfg_close[['Min','Mean','Max']].iloc[-200:].plot(figsize=(10,6), style=['g--','r--','g--'], lw=0.8)\n",
    "\n",
    "pfg_close.Price.iloc[-200:].plot(ax=ax, lw=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = pfg_close[['EWMA']].iloc[-200:].plot(figsize=(10,6), style=['g--'], lw=2.5)\n",
    "\n",
    "pfg_close.Price.iloc[-200:].plot(ax=ax, lw=2.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_close.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pfg_close['SMA50'] = pfg_close.Price.rolling(window=50).mean()\n",
    "pfg_close['SMA250'] = pfg_close.Price.rolling(window=250).mean()\n",
    "\n",
    "pfg_close.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot price in the context of two simple moving averages\n",
    "pfg_close[['Price','SMA50','SMA250']].plot(figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change position when SMA50 crosses SMA250\n",
    "# go long (buy) when SMA50 is greater than SMA250\n",
    "# otherwise, go short (sell)\n",
    "\n",
    "pfg_close.dropna(inplace=True)\n",
    "pfg_close['Position'] = np.where(pfg_close.SMA50 > pfg_close.SMA250, 1, -1)\n",
    "ax = pfg_close[['Price','SMA50','SMA250','Position']].plot(figsize=(10,6), secondary_y='Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vix_close = pd.read_csv('data/^VIX.csv',header=0, index_col=0, parse_dates=True, usecols=['Date','Adj Close'])\n",
    "vix_close.columns = ['VIX']\n",
    "vix_close.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_data = pd.concat([pfg_close[['Price','1 Day Lag','7 Day Lag','30 Day Lag']],vix_close], axis=1, join='inner')\n",
    "corr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rets = np.log(corr_data/corr_data.shift(1))\n",
    "rets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.dropna(inplace=True)\n",
    "rets.plot(subplots=True, figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(rets, figsize=(10,10), diagonal=\"hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity of a Time Series\n",
    "\n",
    "Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations.\n",
    "\n",
    "When a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary to be effective.\n",
    "\n",
    "Classical time series analysis and forecasting methods are concerned with making non-stationary time series data stationary by identifying and removing trends and removing seasonal effects.\n",
    "\n",
    "We will use what we learn from examining the stationarity of the time series as input into the configuration of the modeling algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_close = pd.read_csv('data/PFG.csv',header=0, index_col=0, parse_dates=True, usecols=['Date','Adj Close'])\n",
    "pfg_close.columns = ['Price']\n",
    "pfg_close.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics to Check Stationarity of a Time Series\n",
    "\n",
    "- Using the means and variances of a split dataset to see if there are significant differences\n",
    "- First, need to check that data is normal (follows Gaussian distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfg_close.hist(figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Mean and Variance of Split Dataset\n",
    "\n",
    "- Split the time series into two contiguous sequences. We can then calculate the mean and variance of each group of numbers and compare the values. \n",
    "- The mean and variance values can be different, but if they are close it is an indication the series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pfg_close.Price.values\n",
    "split = round(len(X) / 2)\n",
    "X1, X2 = X[0:split], X[split:]\n",
    "mean1, mean2 = X1.mean(), X2.mean()\n",
    "var1, var2 = X1.var(), X2.var()\n",
    "print('mean1=%f, mean2=%f' % (mean1, mean2))\n",
    "print('variance1=%f, variance2=%f' % (var1, var2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use log transform to see if it improves the variance\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X=np.log(pfg_close.Price.values)\n",
    "split = round(len(X) / 2)\n",
    "X1, X2 = X[0:split], X[split:]\n",
    "mean1, mean2 = X1.mean(), X2.mean()\n",
    "var1, var2 = X1.var(), X2.var()\n",
    "print('mean1=%f, mean2=%f' % (mean1, mean2))\n",
    "print('variance1=%f, variance2=%f' % (var1, var2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller Test\n",
    "\n",
    "Statistical tests can provide a quick check to see whether your time series is stationary or non-stationary.\n",
    "The Augmented Dickey-Fuller test is a type of statistical test called a unit root test. The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend or seasonality.\n",
    "\n",
    "The Null hypothesis of the ADF Test is NON stationary.\n",
    "So we're looking for a p-value <= .05 and a test statistic that is more negative than 1% critical value to reject the null hypothesis of non-stationary, suggesting the data does not have a unit root and is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "## use ADF to check the stationarity.  Null hypothesis is NON stationary.\n",
    "## a p-value <= .05 and a test statistic that is more negative than 1% critical value indicates a stationary series\n",
    "X = pfg_close.Price.values\n",
    "result = adfuller(X)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Decomposition\n",
    "\n",
    "A series is thought to be an aggregate or combination of the four components:\n",
    "\n",
    "- **Level:** The average value in the series.\n",
    "- **Trend:** The increasing or decreasing value in the series.\n",
    "- **Seasonality:** The repeating short-term cycle in the series.\n",
    "- **Noise:** The random variation in the series.\n",
    "\n",
    "All series have a level and noise. The trend and seasonality components are optional.\n",
    "\n",
    "It is helpful to think of the components as combining either **additively** or **multiplicatively**.\n",
    "\n",
    "#### Additive Model\n",
    "An additive model suggests that the components are added together as follows:\n",
    "\n",
    "**y(t) = Level + Trend + Seasonality + Noise**\n",
    "\n",
    "An additive model is linear where changes over time are consistently made by the same amount, with a linear trend that is a straight line and a linear seasonality that has the same frequency (width of cycles) and amplitude (height of cycles).\n",
    "\n",
    "#### Multiplicative Model\n",
    "A multiplicative model suggests that the components are multiplied together as follows:\n",
    "\n",
    "**y(t) = Level * Trend * Seasonality * Noise**\n",
    "\n",
    "A multiplicative model is nonlinear, such as quadratic or exponential with changes increasing or decreasing over time, with a nonlinear trend represented as a curved line, and a non-linear seasonality that has an increasing or decreasing frequency and/or amplitude over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pfg_close.Price\n",
    "series.plot(figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(series, model='addititive', period=1)\n",
    "#decomposition = seasonal_decompose(series, model='multiplicative', period=1)\n",
    "decomposition.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
